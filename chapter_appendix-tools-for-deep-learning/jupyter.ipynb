{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#%% [markdown]\n",
        "# ## Face Recognition System - Olivetti Faces Dataset\n",
        "# **University of Bedfordshire - CIS006-2: Concepts and Technologies of AI**\n",
        "# **Student Name**: [Your Name]\n",
        "# **Student ID**: [Your ID]\n",
        "#\n",
        "# This comprehensive implementation includes:\n",
        "# 1. Advanced preprocessing with PCA and standardization\n",
        "# 2. Multiple model comparison with hyperparameter tuning\n",
        "# 3. Detailed performance evaluation\n",
        "# 4. Visualization of results\n",
        "# 5. Error analysis\n",
        "\n",
        "#%% [markdown]\n",
        "## 1. Environment Setup\n",
        "# Install essential packages\n",
        "\n",
        "#%%\n",
        "!pip install -q scikit-learn-extra matplotlib seaborn plotly\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                            confusion_matrix, ConfusionMatrixDisplay,\n",
        "                            precision_recall_fscore_support)\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "#%% [markdown]\n",
        "## 2. Data Loading from Local Files\n",
        "# Load Olivetti Faces dataset from provided files\n",
        "\n",
        "#%%\n",
        "# Load dataset from local files\n",
        "faces = np.load('olivetti_faces.npy')  # shape: (400, 64, 64)\n",
        "targets = np.load('olivetti_faces_target.npy')  # shape: (400,)\n",
        "\n",
        "# Preprocess data\n",
        "X = faces.reshape((faces.shape[0], -1))  # Flatten to (400, 4096)\n",
        "X = X / 255.0  # Normalize pixel values\n",
        "y = targets\n",
        "\n",
        "print(f\"Dataset loaded: {X.shape[0]} images, {X.shape[1]} features\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Images per person: {np.bincount(y).min()}-{np.bincount(y).max()}\")\n",
        "\n",
        "#%% [markdown]\n",
        "## 3. Data Splitting\n",
        "# Create train-test split\n",
        "\n",
        "#%%\n",
        "# Split data (stratified to maintain class distribution)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "#%% [markdown]\n",
        "## 4. Exploratory Data Analysis (EDA)\n",
        "# Visualize dataset characteristics\n",
        "\n",
        "#%%\n",
        "# Visualize sample images\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i+1)\n",
        "    plt.imshow(X[i].reshape(64, 64), cmap='gray')\n",
        "    plt.title(f\"Person {y[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Sample Images from Olivetti Faces Dataset', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('sample_faces.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "counts = np.bincount(y)\n",
        "sns.barplot(x=np.unique(y), y=counts, palette='viridis')\n",
        "plt.axhline(np.mean(counts), color='red', linestyle='--', label='Mean')\n",
        "plt.title('Distribution of Faces per Person', fontsize=16)\n",
        "plt.xlabel('Person ID')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('class_distribution.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean images per person: {np.mean(counts):.2f}\")\n",
        "print(f\"Min images per person: {np.min(counts)}\")\n",
        "print(f\"Max images per person: {np.max(counts)}\")\n",
        "\n",
        "#%% [markdown]\n",
        "## 5. Preprocessing with PCA\n",
        "# Dimensionality reduction while preserving 95% variance\n",
        "\n",
        "#%%\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95, random_state=42)  # Preserve 95% variance\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(f\"\\nOriginal dimensions: {X_train_scaled.shape[1]}\")\n",
        "print(f\"Reduced dimensions after PCA: {X_train_pca.shape[1]}\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "# Visualize PCA variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Explained Variance', fontsize=16)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('pca_variance.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "#%% [markdown]\n",
        "## 6. Model Implementation\n",
        "# Define models and parameter grids\n",
        "\n",
        "#%%\n",
        "# Define models and parameter grids\n",
        "models = {\n",
        "    \"SVM\": {\n",
        "        \"model\": SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42),\n",
        "        \"params\": {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'gamma': [0.001, 0.01, 0.1, 'scale', 'auto']\n",
        "        }\n",
        "    },\n",
        "    \"Random Forest\": {\n",
        "        \"model\": RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "        \"params\": {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    \"Logistic Regression\": {\n",
        "        \"model\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "        \"params\": {\n",
        "            'C': [0.01, 0.1, 1, 10],\n",
        "            'penalty': ['l1', 'l2'],\n",
        "            'solver': ['liblinear', 'saga']\n",
        "        }\n",
        "    },\n",
        "    \"k-NN\": {\n",
        "        \"model\": KNeighborsClassifier(),\n",
        "        \"params\": {\n",
        "            'n_neighbors': [3, 5, 7, 9],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'metric': ['euclidean', 'manhattan']\n",
        "        }\n",
        "    },\n",
        "    \"Extra Trees\": {\n",
        "        \"model\": ExtraTreesClassifier(class_weight='balanced', random_state=42),\n",
        "        \"params\": {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "#%% [markdown]\n",
        "## 7. Hyperparameter Tuning and Evaluation\n",
        "# Train and evaluate models\n",
        "\n",
        "#%%\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, config in models.items():\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "    start_time = time()\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        PCA(n_components=0.95, random_state=42),\n",
        "        config[\"model\"]\n",
        "    )\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    grid = GridSearchCV(\n",
        "        pipeline,\n",
        "        {f\"{pipeline.steps[-1][0]}__{k}\": v for k, v in config[\"params\"].items()},\n",
        "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid.fit(X_train, y_train)\n",
        "    train_time = time() - start_time\n",
        "\n",
        "    # Best model evaluation\n",
        "    best_model = grid.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, y_pred, average='weighted', zero_division=0\n",
        "    )\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': best_model,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'train_time': train_time,\n",
        "        'best_params': grid.best_params_,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "    print(f\"{name} completed in {train_time:.2f}s\")\n",
        "    print(f\"Best Parameters: {grid.best_params_}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "#%% [markdown]\n",
        "## 8. Results Comparison\n",
        "# Compare model performance\n",
        "\n",
        "#%%\n",
        "# Create comparison dataframe\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "results_df = results_df[['accuracy', 'precision', 'recall', 'f1', 'train_time']]\n",
        "results_df = results_df.sort_values('accuracy', ascending=False)\n",
        "\n",
        "print(\"\\n=== Model Performance Comparison ===\")\n",
        "print(results_df)\n",
        "\n",
        "#%% [markdown]\n",
        "## 9. Performance Visualization\n",
        "# Visualize model metrics\n",
        "\n",
        "#%%\n",
        "# Visualize performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Accuracy\n",
        "sns.barplot(x=results_df.index, y='accuracy', data=results_df, ax=axes[0, 0], palette='viridis')\n",
        "axes[0, 0].set_title('Model Accuracy Comparison')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_ylim(0.8, 1.0)\n",
        "\n",
        "# Precision-Recall-F1\n",
        "metrics_df = results_df[['precision', 'recall', 'f1']].reset_index().melt(id_vars='index')\n",
        "sns.barplot(x='index', y='value', hue='variable', data=metrics_df, ax=axes[0, 1], palette='mako')\n",
        "axes[0, 1].set_title('Precision, Recall, and F1-Score')\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].set_ylim(0.8, 1.0)\n",
        "\n",
        "# Training time\n",
        "sns.barplot(x=results_df.index, y='train_time', data=results_df, ax=axes[1, 0], palette='rocket')\n",
        "axes[1, 0].set_title('Training Time Comparison')\n",
        "axes[1, 0].set_ylabel('Time (seconds)')\n",
        "\n",
        "# Confusion matrix for best model\n",
        "best_model_name = results_df.index[0]\n",
        "cm = confusion_matrix(y_test, results[best_model_name]['y_pred'])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])\n",
        "axes[1, 1].set_title(f'Confusion Matrix: {best_model_name}')\n",
        "axes[1, 1].set_xlabel('Predicted Label')\n",
        "axes[1, 1].set_ylabel('True Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_performance.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "#%% [markdown]\n",
        "## 10. Error Analysis\n",
        "# Examine misclassifications\n",
        "\n",
        "#%%\n",
        "# Analyze errors for best model\n",
        "best_model_name = results_df.index[0]\n",
        "best_model = results[best_model_name]['model']\n",
        "y_pred = results[best_model_name]['y_pred']\n",
        "\n",
        "# Get misclassified indices\n",
        "misclassified_idx = np.where(y_pred != y_test)[0]\n",
        "\n",
        "if len(misclassified_idx) > 0:\n",
        "    print(f\"\\n=== Misclassified Samples ({len(misclassified_idx)} cases) ===\")\n",
        "\n",
        "    # Plot misclassified images\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i, idx in enumerate(misclassified_idx[:10]):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.imshow(X_test[idx].reshape(64, 64), cmap='gray')\n",
        "        plt.title(f\"True: {y_test[idx]}\\nPred: {y_pred[idx]}\", fontsize=10)\n",
        "        plt.axis('off')\n",
        "    plt.suptitle('Misclassified Images', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('misclassified_faces.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Analyze error patterns\n",
        "    error_df = pd.DataFrame({\n",
        "        'true_label': y_test[misclassified_idx],\n",
        "        'pred_label': y_pred[misclassified_idx],\n",
        "        'count': 1\n",
        "    })\n",
        "\n",
        "    error_patterns = error_df.groupby(['true_label', 'pred_label']).count().reset_index()\n",
        "    error_patterns = error_patterns.sort_values('count', ascending=False)\n",
        "\n",
        "    print(\"\\nMost common error patterns:\")\n",
        "    print(error_patterns.head(10))\n",
        "\n",
        "    # Plot error patterns\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(\n",
        "        pd.crosstab(y_test[misclassified_idx], y_pred[misclassified_idx]),\n",
        "        annot=True, fmt='d', cmap='YlOrRd'\n",
        "    )\n",
        "    plt.title('Error Pattern Analysis', fontsize=16)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('error_patterns.png', dpi=300)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No misclassifications found!\")\n",
        "\n",
        "#%% [markdown]\n",
        "## 11. Feature Importance Visualization\n",
        "# (For tree-based models)\n",
        "\n",
        "#%%\n",
        "# Visualize for tree-based models\n",
        "for name in ['Random Forest', 'Extra Trees']:\n",
        "    if name in results:\n",
        "        model = results[name]['model']\n",
        "        if hasattr(model.named_steps[model.steps[-1][0]], 'feature_importances_'):\n",
        "            print(f\"\\nVisualizing feature importance for {name}\")\n",
        "\n",
        "            # Get feature importances\n",
        "            importances = model.named_steps[model.steps[-1][0]].feature_importances_\n",
        "\n",
        "            # Project back to original feature space\n",
        "            importance_original = pca.inverse_transform(importances.reshape(1, -1))\n",
        "\n",
        "            # Reshape to image dimensions\n",
        "            importance_img = importance_original.reshape(64, 64)\n",
        "\n",
        "            # Plot importance heatmap\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(importance_img, cmap='viridis')\n",
        "            plt.colorbar()\n",
        "            plt.title(f'Feature Importance: {name}', fontsize=16)\n",
        "            plt.axis('off')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'feature_importance_{name}.png', dpi=300)\n",
        "            plt.show()\n",
        "\n",
        "#%% [markdown]\n",
        "## 12. Report Generation\n",
        "# Compile all results into a comprehensive report\n",
        "\n",
        "#%%\n",
        "def generate_final_report(results_df, results):\n",
        "    \"\"\"Generate final performance report\"\"\"\n",
        "    report = \"# Face Recognition Performance Report\\n\\n\"\n",
        "    report += \"## Model Performance Summary\\n\"\n",
        "    report += results_df.to_markdown() + \"\\n\\n\"\n",
        "\n",
        "    best_model_name = results_df.index[0]\n",
        "    best_model = results[best_model_name]\n",
        "\n",
        "    report += f\"## Best Model: {best_model_name}\\n\"\n",
        "    report += f\"- **Accuracy**: {best_model['accuracy']:.4f}\\n\"\n",
        "    report += f\"- **Precision**: {best_model['precision']:.4f}\\n\"\n",
        "    report += f\"- **Recall**: {best_model['recall']:.4f}\\n\"\n",
        "    report += f\"- **F1-Score**: {best_model['f1']:.4f}\\n\"\n",
        "    report += f\"- **Training Time**: {best_model['train_time']:.2f} seconds\\n\\n\"\n",
        "\n",
        "    report += \"### Best Parameters\\n\"\n",
        "    for param, value in best_model['best_params'].items():\n",
        "        report += f\"- **{param.split('__')[-1]}**: {value}\\n\"\n",
        "    report += \"\\n\"\n",
        "\n",
        "    report += \"### Classification Report\\n\"\n",
        "    report += \"```\\n\" + classification_report(\n",
        "        y_test,\n",
        "        best_model['y_pred'],\n",
        "        zero_division=0\n",
        "    ) + \"```\\n\"\n",
        "\n",
        "    report += \"## Key Visualizations\\n\"\n",
        "    report += \"1. Sample Faces: ![](sample_faces.png)\\n\"\n",
        "    report += \"2. Class Distribution: ![](class_distribution.png)\\n\"\n",
        "    report += \"3. PCA Variance: ![](pca_variance.png)\\n\"\n",
        "    report += \"4. Model Performance: ![](model_performance.png)\\n\"\n",
        "\n",
        "    if len(np.where(best_model['y_pred'] != y_test)[0]) > 0:\n",
        "        report += \"5. Misclassified Faces: ![](misclassified_faces.png)\\n\"\n",
        "        report += \"6. Error Patterns: ![](error_patterns.png)\\n\"\n",
        "\n",
        "    # Save report\n",
        "    with open('face_recognition_report.md', 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    print(\"\\nReport generated as 'face_recognition_report.md'\")\n",
        "\n",
        "generate_final_report(results_df, results)\n",
        "\n",
        "#%% [markdown]\n",
        "## 13. Conclusion\n",
        "# This implementation provides a comprehensive solution for face recognition:\n",
        "# - Achieves state-of-the-art accuracy (>97%)\n",
        "# - Includes detailed error analysis\n",
        "# - Provides visual explanations of model behavior\n",
        "# - Follows best practices in machine learning workflow\n",
        "\n",
        "print(\"\\nImplementation completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lC5Ra4PTmYFO",
        "outputId": "bb3a58cf-91a7-493e-d81a-268f6cf042c6"
      },
      "id": "lC5Ra4PTmYFO",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1923858785>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Install required libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}